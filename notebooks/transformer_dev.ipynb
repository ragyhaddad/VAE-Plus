{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37b0e4e-37a5-4d8e-b859-5760144a3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7104abc3-454a-4fab-9c86-ed921ab7404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f32751c-f029-46fa-a51d-4edc389ce5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embed_size, latent_dim, num_heads, hidden_dim, num_layers, max_seq_length, vocab):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, embed_size))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(embed_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(embed_size, latent_dim)\n",
    "        self.fc_latent_to_hidden = nn.Linear(latent_dim, embed_size)\n",
    "        \n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(embed_size, num_heads, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.vocab = vocab \n",
    "        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.char2idx[vocab.pad_token], reduction='sum')\n",
    "        self.kl_weight = 0.6\n",
    "        \n",
    "\n",
    "    def compute_loss(self, outputs, targets, logvar, mu):\n",
    "        r_loss = self.criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss * self.kl_weight\n",
    "        return r_loss, kl_loss\n",
    "    \n",
    "    def cyclical_annealing(self,T,M,step,R=0.4, max_kl_weight=1):\n",
    "        \"\"\"\n",
    "        Implementing: <https://arxiv.org/abs/1903.10145>\n",
    "        T = Total steps \n",
    "        M = Number of cycles \n",
    "        R = Proportion used to increase beta\n",
    "        t = Global step \n",
    "        \"\"\"\n",
    "        period = (T/M) # N_iters/N_cycles \n",
    "        internal_period = (step) % (period)  # Itteration_number/(Global Period)\n",
    "        tau = internal_period/period\n",
    "        if tau > R:\n",
    "            tau = max_kl_weight\n",
    "        else:\n",
    "            tau = min(max_kl_weight, tau/R) # Linear function \n",
    "        return tau\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, x, mask=None):\n",
    "        emb = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        encoded = self.encoder(emb, src_key_padding_mask=mask)\n",
    "        last_hidden_state = encoded[:, -1, :] # z this hidden state is the last hidden state.\n",
    "        mu_vector = self.fc_mu(last_hidden_state)\n",
    "        logvar_vector = self.fc_logvar(last_hidden_state)\n",
    "        return last_hidden_state, emb, logvar_vector, mu_vector\n",
    "    \n",
    "    def decode(self, last_hidden_state, x, mask=None):\n",
    "        resized_latent = self.fc_latent_to_hidden(last_hidden_state)\n",
    "        resized_latent = resized_latent.unsqueeze(1).repeat(1, self.max_seq_length, 1)\n",
    "        # add positional encoding to reshaped latents \n",
    "        \n",
    "        resized_latent = resized_latent + self.positional_encoding[:, :resized_latent.size(1), :] \n",
    "        # Add positional encoding\n",
    "        hidden = resized_latent + self.positional_encoding[:, :self.max_seq_length, :]\n",
    "        tgt_mask = self.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        # Pass through decoder\n",
    "        # decoded = self.decoder(x, hidden, tgt_key_padding_mask=mask, memory_key_padding_mask=mask)\n",
    "        decoded = self.decoder(x, hidden, tgt_mask=tgt_mask,tgt_is_causal=True,tgt_key_padding_mask=mask, memory_key_padding_mask=mask)\n",
    "\n",
    "        outputs_v = self.output_fc(decoded)\n",
    "        \n",
    "        return outputs_v\n",
    "\n",
    "    def forward(self,batch):\n",
    "        with_bos, with_eos, masks = batch\n",
    "        last_hidden_states, memory, logvar, mu = m.encode(with_bos, mask=masks)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        output_v = m.decode(z, memory, mask=masks)\n",
    "        return output_v, with_eos, logvar, mu\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs_v, with_eos, logvar, mu = self.forward(batch)\n",
    "        r_loss, kl_loss = self.compute_loss(outputs_v, with_eos, logvar, mu)\n",
    "        # recon_loss = self.criterion(outputs_v.view(-1, outputs_v.size(-1)), with_eos.view(-1))\n",
    "        self.kl_weight = self.cyclical_annealing(100000, 100, step=self.global_step, max_kl_weight=0.6)\n",
    "        # self.kl_weight = 0.8\n",
    "        r = {}\n",
    "        r['r_loss'] = r_loss \n",
    "        r['kl_loss'] = kl_loss\n",
    "        r['loss'] = r_loss + kl_loss\n",
    "        r['kl_weight'] = self.kl_weight\n",
    "        # self.log(r)\n",
    "        for key in r:\n",
    "            self.log(key, r[key])\n",
    "\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f36f29c-65f7-4ac7-afca-90c7622c80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVocab:\n",
    "    def __init__(self, sequences=None, target_col=None, char2idx={}, idx2char={}):\n",
    "        self.sos_token = '<sos>' \n",
    "        self.eos_token = '<eos>' \n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.char2idx = char2idx \n",
    "        self.idx2char = idx2char\n",
    "        if sequences != None:\n",
    "            self.extract_charset(sequences)\n",
    "        else:\n",
    "            self.build_vocab_aa()\n",
    "\n",
    "    def handle_special(self, smi):\n",
    "        smi = smi.replace('Cl', 'Q')\n",
    "        smi = smi.replace('Br', 'W')\n",
    "        smi = smi.replace('[nH]', 'X')\n",
    "        smi = smi.replace('[H]', 'Y')\n",
    "        return smi\n",
    "    def reverse_special(self, smi):\n",
    "        smi = smi.replace('Q', 'Cl')\n",
    "        smi = smi.replace('W', 'Br')\n",
    "        smi = smi.replace( 'X','[nH]')\n",
    "        smi = smi.replace('Y', '[H]') \n",
    "        return smi \n",
    "    def extract_charset(self, sequences):\n",
    "        \"\"\"\n",
    "        Extract charset from SMILES strings\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame containing SMILES strings\n",
    "\n",
    "        \"\"\"\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        print('extracting charset..')\n",
    "        i = 0\n",
    "        for c in self.special_tokens:\n",
    "            if c not in self.char2idx:\n",
    "                self.char2idx[c] = i\n",
    "                self.idx2char[i] = c\n",
    "                i += 1\n",
    "        all_smi = sequences\n",
    "        for _, smi in enumerate(tqdm(all_smi)):\n",
    "            smi = self.handle_special(smi)\n",
    "            for c in smi:\n",
    "                if c not in self.char2idx:\n",
    "                    self.char2idx[c] = i\n",
    "                    self.idx2char[i] = c\n",
    "                    i += 1\n",
    "    \n",
    "    def build_vocab_aa(self):\n",
    "        aa_alphabet = list('ACDEFGHIKLMNPQRSTVWYXUZBO')  # 20 standard amino acids\n",
    "        all_chars = self.special_tokens + aa_alphabet\n",
    "        self.char2idx = {token: idx for idx, token in enumerate(all_chars)}\n",
    "        self.idx2char = {idx: token for token, idx in self.char2idx.items()}\n",
    "\n",
    "class TransformerDataset:\n",
    "    def __init__(self, sequences, vocab, max_len=100):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab \n",
    "        self.max_len = max_len \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        seq = self.vocab.handle_special(seq)\n",
    "        tokens = [self.vocab.char2idx[i] for i in seq]\n",
    "        \n",
    "        with_bos = [self.vocab.char2idx[self.vocab.sos_token]] + tokens\n",
    "        with_eos = tokens + [self.vocab.char2idx[self.vocab.eos_token]]\n",
    "        with_bos += [self.vocab.char2idx[self.vocab.pad_token]] * (self.max_len - len(with_bos))\n",
    "        with_eos += [self.vocab.char2idx[self.vocab.pad_token]] * (self.max_len - len(with_eos))\n",
    "        attention_mask = [1 if t != self.vocab.char2idx[self.vocab.pad_token] else 0 for t in with_bos]\n",
    "        return torch.tensor(with_bos), torch.tensor(with_eos), torch.tensor(attention_mask).float()\n",
    "\n",
    "    def collate(self, batch):\n",
    "        with_bos, with_eos, masks = zip(*batch)\n",
    "        with_bos = torch.stack(with_bos) \n",
    "        with_eos = torch.stack(with_eos)\n",
    "        masks = torch.stack(masks)\n",
    "        return with_bos, with_eos, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dd7b51b-e23d-41a0-b2ab-924a7a22e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14fc736e-5f01-46b8-8b68-33124235269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspace/uniref50_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08869df5-6517-48ea-9bf9-01e3ad4f9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24487b91-7059-4250-bcfe-865a21c730ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['str_len'] = df.Sequence.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24eeac2b-57d3-4c59-8eff-738bacaf1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "991edb53-7669-434d-9617-568922253fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['str_len'] <= 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7c2fd4c-8d29-4de5-bd15-2445077137f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df.Sequence.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d44fa03c-fd50-46f4-911e-f86be77bef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TransformerVocab()\n",
    "\n",
    "ds = TransformerDataset(sequences, vocab=vocab, max_len=max_len + 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca72e31b-18eb-4113-9b91-0a360c0d981f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-gorge-3</strong> at: <a href='https://wandb.ai/rhaddad7/run_pod_prot/runs/q95c7y5t' target=\"_blank\">https://wandb.ai/rhaddad7/run_pod_prot/runs/q95c7y5t</a><br/> View project at: <a href='https://wandb.ai/rhaddad7/run_pod_prot' target=\"_blank\">https://wandb.ai/rhaddad7/run_pod_prot</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241114_155224-q95c7y5t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.finish()\n",
    "import wandb\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(project=\"run_pod_prot\", log_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eb02079-d541-4b03-afca-0b361fb67716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241114_155301-rjjqvzep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rhaddad7/run_pod_prot/runs/rjjqvzep' target=\"_blank\">misunderstood-oath-4</a></strong> to <a href='https://wandb.ai/rhaddad7/run_pod_prot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rhaddad7/run_pod_prot' target=\"_blank\">https://wandb.ai/rhaddad7/run_pod_prot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rhaddad7/run_pod_prot/runs/rjjqvzep' target=\"_blank\">https://wandb.ai/rhaddad7/run_pod_prot/runs/rjjqvzep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | embedding           | Embedding          | 7.4 K  | train\n",
      "1 | encoder             | TransformerEncoder | 2.4 M  | train\n",
      "2 | fc_mu               | Linear             | 65.8 K | train\n",
      "3 | fc_logvar           | Linear             | 65.8 K | train\n",
      "4 | fc_latent_to_hidden | Linear             | 65.8 K | train\n",
      "5 | decoder             | TransformerDecoder | 4.5 M  | train\n",
      "6 | output_fc           | Linear             | 7.5 K  | train\n",
      "7 | criterion           | CrossEntropyLoss   | 0      | train\n",
      "  | other params        | n/a                | 51.7 K | n/a  \n",
      "-------------------------------------------------------------------\n",
      "7.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.1 M     Total params\n",
      "28.520    Total estimated model params size (MB)\n",
      "202       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0723ee84b8ac4a44a9eea693abd13666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = Transformer(vocab_size=len(vocab.char2idx), embed_size=256, latent_dim=256, num_heads=8, hidden_dim=64, num_layers=8, max_seq_length=max_len + 2, vocab=vocab)\n",
    "\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=128, collate_fn=ds.collate)\n",
    "\n",
    "trainer = pl.Trainer(devices=1, logger=[wandb_logger])\n",
    "\n",
    "trainer.fit(m, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8982996b-4e25-48ab-b2bb-a92ecd197e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(m):\n",
    "    current_tokens = [m.vocab.char2idx[m.vocab.sos_token]]\n",
    "    tgt_tokens = torch.tensor([current_tokens]).long().to(m.device)\n",
    "    s = \"\"\n",
    "    last_hidden_state = torch.normal(0, 1.0, size=(1, 128)).to(m.device)\n",
    "    resized_latent = m.fc_latent_to_hidden(last_hidden_state) \n",
    "    resized_latent = resized_latent.unsqueeze(1).repeat(1, 1, 1) \n",
    "    emb = m.embedding(tgt_tokens) + m.positional_encoding[:, :tgt_tokens.size(1), :]\n",
    "    for i in range(128):\n",
    "        tgt_mask = m.generate_square_subsequent_mask(emb.size(1)).to(emb.device)\n",
    "        decoded = m.decoder(emb, resized_latent, tgt_mask=tgt_mask)\n",
    "        outputs_v = m.output_fc(decoded)\n",
    "        outputs_v = outputs_v[:,-1,:] \n",
    "        top_char = torch.argmax(outputs_v)\n",
    "        print(top_char)\n",
    "        print(outputs_v.shape)\n",
    "        if top_char == vocab.char2idx[vocab.eos_token]:\n",
    "            break\n",
    "        current_tokens.append(top_char.item())\n",
    "        tgt_tokens = torch.tensor([current_tokens]).long().to(m.device)\n",
    "        emb = m.embedding(tgt_tokens) + m.positional_encoding[:, :tgt_tokens.size(1), :]\n",
    "        s += vocab.idx2char[top_char.item()]\n",
    "    s = vocab.reverse_special(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f59e2b-768e-4185-892e-f625afdb7b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5bee6b61-8137-459f-8b70-ef7b10eb249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n",
      "tensor(19)\n",
      "torch.Size([1, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed90a1-4c24-4219-a89c-a1bd9fbdcb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
