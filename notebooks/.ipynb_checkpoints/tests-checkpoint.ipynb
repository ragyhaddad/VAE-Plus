{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26e04b1-4aa4-4373-80f2-1facf29c7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddc7d41-af3d-45c3-9b25-07690151fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_cyc as vc\n",
    "import pandas as pd \n",
    "import lightning as pl\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7ef8b2-8af5-457c-b3b1-40fcc0c78e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/git-repos/datasets/peptides_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92092b0-7577-4973-ba3a-f5cb850a9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting charset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 58094/58094 [00:00<00:00, 82491.39it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = vc.Vocab(df, 'Sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b7de7d-fa7e-41f9-8b6d-116e0346ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 58094/58094 [00:00<00:00, 77071.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = vc.Dataset(df, vocab, 'Sequence',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0236d7-59c1-4514-b77e-6f4386f10ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset,batch_size=32,shuffle=True, collate_fn=dataset.collate,num_workers=1)\n",
    "val_dl = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=dataset.collate, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a46d83d-8e3c-4db4-9067-5f3849f1a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca7a206-6939-4155-a22f-0af66aba54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vc.VAE(32,32,2, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaba5d-ce99-4c31-8202-4fb15279c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:72: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "/opt/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | encoder       | GRU              | 31.5 K | train\n",
      "1 | decoder       | GRU              | 15.7 K | train\n",
      "2 | mu_fc         | Linear           | 4.1 K  | train\n",
      "3 | logvar_fc     | Linear           | 4.1 K  | train\n",
      "4 | embedding     | Embedding        | 896    | train\n",
      "5 | latent2hidden | Linear           | 2.1 K  | train\n",
      "6 | outputs2vocab | Linear           | 924    | train\n",
      "7 | criterion     | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------------\n",
      "59.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "59.4 K    Total params\n",
      "0.238     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/ragyhaddad/git-repos/vae_cyc/vae_cyc/rnn.py:96: UserWarning: MPS: nonzero op is supported natively starting from macOS 14.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:361.)\n",
      "  prob[(x_mutate - self.vocab.sos_idx) * (x_mutate - self.vocab.eos_idx) == 0] = 1\n",
      "/opt/miniconda3/lib/python3.12/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/opt/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af1898391f48fca91f6aea27573fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(7391.3740, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7369.8623, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(21.5117, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7346.8257, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7346.8257, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(0., device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7324.7310, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7324.3960, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(0.3350, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7338.4512, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7337.6821, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(0.7692, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7469.2686, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7468.0972, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(1.1712, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7536.5605, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7534.6904, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(1.8701, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7199.6792, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7196.9717, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(2.7077, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6049.8384, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6045.9795, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(3.8587, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6765.1792, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6760.1670, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(5.0122, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6933.7705, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6926.4185, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(7.3521, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7603.3481, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7593.9639, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(9.3843, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6585.1196, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6573.1538, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(11.9657, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7057.4028, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7041.8350, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(15.5678, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6673.5381, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6654.4507, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(19.0872, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6971.1650, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6946.2764, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(24.8888, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6567.9429, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6539.1641, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(28.7789, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7104.3423, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7069.8340, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(34.5081, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6508.0249, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6468.4004, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(39.6245, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7147.3477, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7102.4492, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(44.8985, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7051.7344, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7004.9424, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(46.7920, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6757.7998, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6711.2891, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(46.5110, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6764.2344, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6716.5176, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(47.7168, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6643.3096, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6595.4199, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(47.8899, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6977.3130, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6931.1431, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(46.1700, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7461.9087, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7416.5552, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(45.3535, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6872.8340, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6831.5908, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(41.2433, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6336.9072, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6300.2373, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(36.6702, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6529.6470, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6495.6787, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(33.9682, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6866.3218, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6835.6245, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(30.6973, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7348.1763, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7320.2666, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(27.9098, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6096.6699, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6070.6084, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(26.0615, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6874.5283, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6850.4194, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(24.1088, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(5807.8359, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(5787.4424, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(20.3935, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6634.8623, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6615.5869, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(19.2752, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6516.2847, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6497.8955, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(18.3894, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6505.7979, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6488.0005, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(17.7972, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7048.6353, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7032.2461, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(16.3893, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6174.6206, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6160.5801, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(14.0403, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(5313.0278, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(5299.5386, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(13.4894, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6725.3052, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6712.4829, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(12.8224, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(5889.2651, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(5876.6802, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(12.5851, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7271.3853, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7259.9434, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(11.4420, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6802.8125, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6791.4707, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(11.3416, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6776.0435, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6765.2910, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.7524, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6533.4517, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6522.6587, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.7931, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6945.1587, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6934.0723, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(11.0864, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(7197.7944, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(7187.6768, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.1176, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6629.1826, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6618.5161, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.6666, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6823.6592, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6813.6973, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(9.9617, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6840.2793, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6830.1963, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.0832, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n",
      "{'loss': tensor(6669.4453, device='mps:0', grad_fn=<AddBackward0>), 'r_loss': tensor(6659.2202, device='mps:0', grad_fn=<NllLossBackward0>), 'kl_loss': tensor(10.2253, device='mps:0', grad_fn=<MulBackward0>), 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bcd67-ce9c-4e8f-a649-812b488c1895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
